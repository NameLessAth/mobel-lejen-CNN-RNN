# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUIvJsgLnDwQ5TaLNh-mzfv_y-ihFaVC
"""

import tensorflow as tf
import keras
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder

val = pd.read_csv("https://drive.google.com/uc?id=1D0JXlwAMhJnIQYZhhLdYyLFG9GvFGMuV").drop(columns=['id'])
train = pd.read_csv("https://drive.google.com/uc?id=10dYq4_oAodarAjTQb2rzhB4kjeIsBnK4").drop(columns=['id'])
test = pd.read_csv("https://drive.google.com/uc?id=1XA1gknpv4n5cruCHikt4sfuhUCXWqzrg").drop(columns=['id'])
display(train.head())
display(test.head())
display(val.head())

# one hot encoding
label_encoder = LabelEncoder()
Y_train = label_encoder.fit_transform(train['label'])
Y_val = label_encoder.transform(val['label'])

x_train = train['text'].values
x_val = val['text'].values

display(Y_train)
display(x_train)

# text vectorization
vectorizer = keras.layers.TextVectorization(
    max_tokens=10000,
    output_mode='int',
    output_sequence_length=10
)

vectorizer.adapt(x_train)
print(vectorizer.get_vocabulary()[:10])

model = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.SimpleRNN(64),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history = model.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training vs Validation Loss')
plt.show()

# Variasi depth
model1 = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.SimpleRNN(64),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model1.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history1 = model1.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

model2 = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.SimpleRNN(64),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model2.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history2 = model2.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

model3 = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.SimpleRNN(64),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model3.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history3 = model3.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

plt.plot(history1.history['loss'], label='Training Loss 1')
plt.plot(history1.history['val_loss'], label='Validation Loss 1')
plt.plot(history2.history['loss'], label='Training Loss 2')
plt.plot(history2.history['val_loss'], label='Validation Loss 2')
plt.plot(history3.history['loss'], label='Training Loss 3')
plt.plot(history3.history['val_loss'], label='Validation Loss 3')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training vs Validation Loss')
plt.show()

"""# Kesimpulan
Semakin banyak layer RNN, semakin kompleks modelnya, sehingga training loss lebih rendah. Namun, jika layer terlalu banyak, dapat terjadi overfitting (terlihat dari validation loss dengan 4 layer lebih tinggi dari validation loss dengan 2 dan 3 layer)
"""

# Variasi width
model11 = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.SimpleRNN(64),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model11.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history11 = model1.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

model12 = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.SimpleRNN(128, return_sequences=True),
    keras.layers.SimpleRNN(128),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model12.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history12 = model12.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

model13 = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.SimpleRNN(256, return_sequences=True),
    keras.layers.SimpleRNN(256),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model13.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history13 = model13.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

plt.plot(history11.history['loss'], label='Training Loss 1')
plt.plot(history11.history['val_loss'], label='Validation Loss 1')
plt.plot(history12.history['loss'], label='Training Loss 2')
plt.plot(history12.history['val_loss'], label='Validation Loss 2')
plt.plot(history13.history['loss'], label='Training Loss 3')
plt.plot(history13.history['val_loss'], label='Validation Loss 3')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training vs Validation Loss')
plt.show()

"""# Kesimpulan
Semakin banyak sel RNN, semakin tinggi kompleksitas model. Namun, jika terlalu banyak, dapat menyebabkan _instability_ (bagian naik-turun di awal validation loss 2 dan 3), dan overfitting (validation loss 3).
"""

# Variasi arah
model21 = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.SimpleRNN(32),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model21.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history21 = model21.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

model22 = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero = True),
    keras.layers.Bidirectional(keras.layers.SimpleRNN(64, return_sequences=True)),
    keras.layers.Bidirectional(keras.layers.SimpleRNN(32)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(3, activation='softmax')
 ])
model22.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
history22 = model22.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

plt.plot(history21.history['loss'], label='Training Loss 1')
plt.plot(history21.history['val_loss'], label='Validation Loss 1')
plt.plot(history22.history['loss'], label='Training Loss 2')
plt.plot(history22.history['val_loss'], label='Validation Loss 2')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training vs Validation Loss')
plt.show()

"""# Kesimpulan
Bidirectional layer dapat menghasilkan model yang lebih kompleks, sehingga training dapat menghasilkan validation loss yang lebih rendah (model 2).
"""

# membuat model yang bagus untuk disimpan weightnya
modelfinal = tf.keras.Sequential([
    vectorizer,
    keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=128, mask_zero = True),
    keras.layers.Bidirectional(keras.layers.SimpleRNN(64, return_sequences=True)),
    keras.layers.SimpleRNN(64, return_sequences=True),
    keras.layers.Dropout(0.25),
    keras.layers.Bidirectional(keras.layers.SimpleRNN(32)),
    keras.layers.Dropout(0.25),
    keras.layers.Dense(3, activation='softmax')
 ])
modelfinal.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy())
historyfinal = modelfinal.fit(x_train, Y_train, epochs=50, validation_data=(x_val, Y_val))

plt.plot(historyfinal.history['loss'], label='Training Loss')
plt.plot(historyfinal.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training vs Validation Loss')
plt.show()

y_pred = modelfinal.predict(x_val)
#f1 score
dummy = pd.get_dummies(val['label'], prefix='label')
f1score = tf.keras.metrics.F1Score(threshold=0.5)
f1score.update_state(dummy, y_pred)
result = f1score.result()
print(result)